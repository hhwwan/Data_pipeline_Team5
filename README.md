# End-to-end 데이터 파이프라인 구성하기
## 주제: Airflow와 dbt를 활용한 영화 산업 데이터 분석
## Programmers 데브코스 Data Engineering 5기
- <b>Team</b>`하이파이브`
  - <b>전병운 `Airflow와 DBT를 이용한 데이터 ETL 작업 및 ELT작업 및 스크린수와 영화 흥행의 상관관계 분석 대시보드 제작`</b>
  - <b>김동환 `Airflow와 DBT를 이용한 데이터 ETL 작업 및 ELT작업 및 관객수와 매출액의 상관관계 분석 대시보드 제작`</b>
  - <b>설연수 `Airflow와 DBT를 이용한 데이터 ETL 작업 및 ELT작업 및 시즌별 장르와 관객 수의 상관관계 분석 대시보드 제작`</b>
  - <b>최은주 `Airflow와 DBT를 이용한 데이터 ETL 작업 및 ELT작업 및 영화사, 영화 배우 관련 대시보드 제작`</b>
- **프로젝트 진행 기간**: 2025.01.13 ~ 2025.02.05

## 목차
###### 1. 프로젝트 개요
###### 2. 프로젝트 주제 선정 이유
###### 3. 프로젝트 세부 결과
###### 4. 활용 기술 및 프레임 워크
###### 5. 기대 효과
###### 6. 프로젝트 결론

## 1. 프로젝트 개요
이번 프로젝트는 기존 프로젝트의 구조를 개선하여 데이터 처리 및 분석 자동화를 강화하는 방향으로 진행되었습니다. 

기존 프로젝트에서는 **Amazon Lambda와 EventBridge**를 활용하여 데이터 API에서 원본 데이터를 가져오고, Redshift를 데이터 웨어하우스로 활용하여 데이터 웨어하우스와 데이터 마트를 구축한 뒤 **Preset을 통해 시각화**하는 구조였습니다. 

하지만 이번 프로젝트에서는 **Airflow를 활용한 데이터 수집 자동화 및 의존성 관리**와 **dbt를 통한 데이터 변환 자동화**를 추가하였습니다.

또한 **Docker 컨테이너를 사용하여 환경을 일관성 있게 유지**하고, **EC2 서버에서 Airflow를 실행**하는 방식으로 아키텍처를 발전시켰습니다.

## 2. 프로젝트 주제 선정 이유
기존 프로젝트에서 데이터 파이프라인 자동화 수준이 제한적이었으며, 각 단계의 의존성을 체계적으로 관리할 수 있는 방법이 부족했습니다. 

이를 해결하기 위해 Airflow를 도입하여 데이터 수집 및 변환을 자동화하고, dbt를 활용하여 데이터 변환 및 검증을 자동화함으로써 데이터 처리 효율성을 극대화하고자 했습니다.

## 3. 프로젝트 세부 결과
<img src="https://github.com/user-attachments/assets/894c6bbf-305e-41a1-b6b4-f2c5503310b8"  width="1000" height="500"/><br>

### 1. 데이터 수집 및 자동화 프로세스
#### 1) EC2 기반 환경 구성
- Amazon EC2 인스턴스를 활용하여 Airflow Cluster 운영.
- Docker 컨테이너 환경을 도입하여 개발 환경과 운영 환경의 일관성을 유지.
- Docker를 통해 각종 애플리케이션(Airflow, dbt 등)을 독립적으로 실행하며, 확장성과 안정성을 확보.
- ETL 과정 전반에서 필요한 애플리케이션을 Docker로 관리하여 설정 및 배포 효율성을 향상.

#### 2) Airflow의 역할
- Apache Airflow를 통해 데이터 수집, 변환, 적재 과정을 DAG(Directed Acyclic Graph)로 관리.
- 주요 역할:
  - 스케줄 기반 실행: 지정된 일정에 따라 데이터 수집 및 변환 작업을 자동화.
  - 태스크 의존성 관리: 데이터 파이프라인의 모든 단계를 명확히 정의하고, 순차적으로 실행.
  - 복구 용이성: 실패한 태스크를 Backfill 기능으로 쉽게 재실행 가능.
  - 효율적인 운영: DAG 기반의 시각적 관리 도구로, 작업 흐름과 문제 지점을 한눈에 파악 가능.

#### 3) 박스오피스 ETL 파이프라인 흐름
데이터 수집부터 적재까지의 주요 단계는 다음과 같습니다:
- 데이터 수집
  - KOFIC(영화진흥위원회) API를 통해 일별 박스오피스 데이터를 수집
- Parquet 변환 및 S3 업로드
  - 변환된 데이터를 Parquet 형식으로 저장.
  - S3 버킷(`airflow-project3`)에 업로드하여 저장소로 활용.
- 데이터 적재
  - S3에 저장된 Parquet 데이터를 Redshift로 복사. ⇒ 어떤 방식으로 복사하였는가
  - 분석에 필요한 원천 데이터를 Redshift의 `raw_data` 스키마에 적재

#### 4) 통합 데이터 파이프라인 흐름도
1. Data Extraction
   [KOFIC API]

2. Save as Parquet
   [Parquet Conversion & S3 Upload]

3. Load to Redshift
   [Redshift Data Load]

#### 5) 주요 개선 사항
- CSV -> Parquet 전환
  - Parquet은 대규모 데이터 처리에서 저장 공간과 읽기 성능이 뛰어나, 기존 CSV보다 효율적.
- Airflow 기반 자동화
  - 데이터 수집, 변환, 적재의 모든 과정을 자동화.
- 환경의 일관성
  - EC2와 Docker를 활용하여 환경 간 차이를 제거하고, 배포와 유지보수를 간소화.

### 2. 데이터 저장 및 변환
#### 1) 데이터 저장 및 테이블 관리
- raw_data 스키마
  - KOFIC API에서 수집한 원본 데이터를 Parquet 파일로 변환하여 S3에 저장.
  - S3에 저장된 원본 데이터를 Redshift의 `raw_data` 스키마로 적재.
  - 원본 데이터를 유지하여 후속 데이터 변환 및 분석을 위한 기초 데이터로 활용.
- staging 스키마
  - `raw_data` 스키마의 데이터를 정제하고, 필요한 컬럼을 추가 및 변환하여 저장.
  - 중복 제거 등 데이터 전처리 수행.
- analytics 스키마
  - 최종 분석 및 시각화를 위해 변환된 데이터를 저장.
  - BI 대시보드에서 효율적으로 조회할 수 있도록 구조화된 테이블 제공.
  
#### 2) DBT활용
데이터 변환은 dbt를 활용하여 SQL 기반으로 수행되며, 데이터 모델링과 변환 과정을 체계적으로 관리합니다. 

특히, DBT는 Airflow DAG과 통합되어 매일 실행되며, 데이터를 변환합니다.

1. DBT 모델 실행
    - `raw_data` → `staging` → `analytics` 스키마로 데이터를 순차적으로 변환.
2. 모듈화된 데이터 변환
    - DBT 모델은 여러 단계의 모듈로 구성되며, 중복 제거, 집계, 필터링 등의 작업이 각 단계별로 이루어짐.

#### 3) 새로운 구조와 기존 구조 비교
##### **기존 구조**

- 데이터 저장:
  - CSV 파일 형태로 S3에 저장.
  - 데이터 적재 후 수동 SQL 작업으로 데이터 전처리 수행.
- 데이터 변환:
    - 변환 작업이 비표준화되어 유지보수와 확장성이 부족.

##### **새로운 구조**

- 데이터 저장:
  - 데이터를 **Parquet 포맷**으로 저장하여 압축 효율과 성능 향상.
  - Airflow DAG으로 Parquet 파일 자동 생성 및 S3 업로드.
- 데이터 변환:
  - DBT를 활용해 변환 과정을 SQL 기반으로 표준화.
  - Airflow와 통합하여 변환 작업 자동화 및 일정 관리.

### 3. 성과 및 장점
#### 1) Airflow 및 Parquet 도입으로 데이터 수집 자동화 수준 향상
- Airflow를 활용하여 데이터 수집, 변환, 적재 작업을 자동화:
  - DAG를 통해 태스크 간의 의존성 관리.
  - 특정 작업 실패 시 retry를 통해 신속한 복구 및 재실행 가능.
- Parquet 포맷 도입으로 저장 효율성 및 읽기 성능 향상:
  - 기존 CSV 대비 저장 공간 절약 및 대규모 데이터 처리 성능 개선.
  - Parquet 포맷을 통한 S3 대상 Redshift 쿼리 성능 증가

#### 2) dbt 도입으로 데이터 변환 프로세스 개선
- SQL 기반의 데이터 변환 자동화:
  - dbt를 활용하여 `raw_data` → `staging` → `analytics` 스키마로 체계적인 데이터 변환을 수행.
  - 데이터 정제, 중복 제거, 지표 계산 등의 변환 작업이 표준화되어 유지보수 용이.
- Airflow와 dbt 통합:
  - Airflow DAG에서 dbt 작업을 자동으로 트리거하여 데이터 파이프라인의 일관성 확보.
  - 동적 테이블 생성 및 데이터 변환이 가능하도록 설정하여 분석 효율성 향상.

#### 3) Docker 기반의 개발 및 운영 환경 표준화
- Docker 컨테이너 환경을 통해 운영 환경과 개발 환경의 차이를 제거:
  - Airflow와 dbt를 포함한 모든 애플리케이션을 Docker로 관리하여 배포 및 확장 용이.
  - EC2 인스턴스에서 실행하여 안정성과 확장성을 동시에 확보.
- 컨테이너를 활용한 애플리케이션 격리로 설정 오류 및 환경 의존성 문제 최소화.

#### 4) 데이터 검증 및 품질 관리 강화
- dbt의 테스트 기능을 활용하여 데이터 무결성을 자동으로 확인:
  - 주요 데이터 컬럼에 대한 값 검증 및 예상치 못한 오류 탐지.
  - 데이터 정합성 문제를 사전에 방지하여 분석 신뢰성 향상.
- Parquet 기반 데이터 저장으로 스키마 일관성을 유지하며 데이터 무결성 확보.

#### 5) 운영 비용 절감 및 분석 속도 향상
- 기존 수작업 기반 데이터 수집 및 변환 과정에서 자동화 기반으로 전환:
  - 운영 시간 단축 및 리소스 효율성 극대화.
  - 데이터 변환과 적재 과정에서 발생하는 비용 절감.
- Redshift와 S3의 효율적 연동으로 대규모 데이터의 분석 속도 향상.

#### 6) 시각화 효율성 증가
- 최종 분석 데이터를 Preset.io로 시각화하여 데이터 기반 의사결정을 지원.
- 데이터 변환이 표준화됨에 따라 시각화 대시보드의 정확성과 활용도가 증가.

### 4. 시각화 결과
<img src="https://github.com/user-attachments/assets/5d463e2f-585a-41ba-843f-ce8b76ee3214"  width="1000" height="1600"/><br>
### 주제 : 스크린수와 영화 흥행의 상관관계 분석

### **📊 1. 스크린 수 vs 매출액 (Scatter Plot)**

- **필요성**:스크린 수가 많아질수록 영화 매출이 증가하는지, **스크린 배정이 흥행에 미치는 영향을 분석**하기 위해 필요
- **목적**:영화별 스크린 수와 매출 간의 **상관관계**를 확인하여, 배급 전략과 효율성을 평가

### **📊 2. 스크린 수 vs 관객 수 (Scatter Plot)**

- **필요성**:스크린 수와 관객 수 간의 관계를 분석하여, 관객 수 증가에 스크린 배정이 어떤 역할을 하는지 확인
- **목적**:스크린 수가 많아질수록 **실제 관객 유입에 기여**하는지를 평가

### **📊 3. 영화별 스크린 수 & 매출액 비교 (Bar Chart)**

- **필요성**:상위 10개 영화를 대상으로 스크린 배정과 매출액을 비교해, 영화별 **스크린 활용 효율성**을 분석
- **목적**:특정 영화가 적절한 스크린 수를 배정받았는지, 과소/과대 배정 여부를 파악

---

### 주제: 관객수와 매출액의 상관관계 분석

### **📊** 1. 영화별 매출액과 관객수의 상관관계 (Mixed chart)

- **필요성**: 관객수가 많을수록 영화 매출이 증가하므로 관객수 홍보가 영화의 전체적인 매출에 영향을 많이 끼친다는 분석을 확인하기 위해 필요
- **목적**: 영화별 매출액과 관객수의 상관관계를 확인하여, 관객수가 많을수록 매출액이 높다는 관계 확인

### **📊** 2. 날짜별 관객수와 매출액의 상관관계 (Mixed chart)

- **필요성**: 각 날짜별로 관객수가 많을수록 영화 매출이 증가한다는 분석을 확인하기 위해 필요
- **목적:** 일주일 중 어느날짜(요일)이 평균적으로 가장 많은 매출을 올리는지 확인하여 특정 요일에 홍보하는 것이 매출에 더 좋은 영향을 끼칠 수 있다는 것을 확인

### **📊** 3. 최근 일주일 매출현황 (Big Number with Trendline)

- **필요성**: 가장 최근의 매출을 숫자로 보기 쉽게 확인하기 위해 필요 및 최근 일주일의 매출동향을 그래프로 파악하기 위해 필요
- **목적**: 최근 일주일간의 매출 성과를 확인하기위해

### **📊 4**.  영화 매출 & 관객 수 분석 시각화 (Bubble Chart)

- **필요성**: 영화별 매출과 관객 수의 관계 분석, 영화의 흥행 분석
- **목적**: 근 1년 간 흥행한 영화들을 알고 매출과 관객 수 비율 판단 가능

---

### 주제 : 시즌별(기념일별) 장르와 관객 수의 상관관계 분석

### **📊 1. 시즌 별 장르 관객 수 (Bar Chart)**

- **필요성**:시즌 마다 흥행하는 영화 장르를 분석하고 시즌 마다 어떤 장르가 관객 수에 영향을 미치는지 확인하기 위함
- **목적**: 영화 장르에 따라 개봉 시기를 결정하는데 도움이 되는 지표로 활용 할 수 있음

### **📊 2. 시즌 별 장르 관객 수 (Pivot Table)**

- **필요성**:영화 장르와 시즌 별 관객 수를 한눈에 보기 위해 테이블 형식으로 시각화 함
- **목적**: 영화 장르에 따라 개봉 시기를 결정하는데 도움이 되는 지표로 활용 할 수 있음

### **📊 3. 기념일 별 장르 관객 수 (Pie Chart)**

- **필요성**:기념일 마다 관객들이 선호하는 장르를 확인하고 분석하기 위함
- **목적**: 기념일에 관객들이 선호하는 장르를 분석하고 영화 홍보, 마케팅에 도움이 되는 지표로 활용 할 수 있음

## 4. 활용 기술 및 프레임 워크

### 데이터 수집
`Apache Airflow` `KOFIC API`

### 데이터 저장
`Redshift`

### 데이터 변환
`dbt (Data Build Tool)`

### 자동화 및 환경 구축
`Docker` `EC2`

### 시각화
`Preset io`

## 5. 기대 효과
### ✅ **데이터 파이프라인의 확장성과 유지보수 용이성 증가**

- 기존에는 **Lambda 기반의 개별 실행 방식**이었지만,Airflow 도입으로 **데이터 수집 및 변환 전체 프로세스를 하나의 DAG으로 관리** 가능.
- 데이터 수집, 저장, 변환 단계 간 **의존성 관리 기능을 통해 유지보수 효율성 증대**.

### ✅ **데이터 처리 자동화 수준 향상**

- 기존에는 **수작업이 필요했던 데이터 변환 및 검증**을 dbt를 통해 자동화하여 **운영 부담 감소**.
- **데이터 품질 검증을 자동으로 수행**하여 신뢰성 향상.

### ✅ **시각화 결과의 정확성 및 활용성 증가**

- **데이터가 최신 상태로 유지되는 시각화 대시보드 제공**.
- **데이터 검증이 강화된 분석 데이터를 제공**하여 보다 신뢰할 수 있는 인사이트 도출 가능.

## 6. 프로젝트 결론
기존 프로젝트 대비 **데이터 수집, 변환, 저장의 자동화 수준을 향상**시켰습니다.

Airflow를 활용한 **데이터 파이프라인 자동화**, dbt를 활용한 **데이터 변환 자동화**, Docker 기반의 **개발/운영 환경 일관성 확보**를 통해 **운영 비용 절감 및 유지보수 용이성 개선**을 달성하였습니다. 

이러한 개선을 통해 **보다 안정적이고 확장 가능한 데이터 분석 환경을 구축**하였으며, 데이터 시각화 결과의 신뢰성과 정확도를 크게 향상시킬 수 있었습니다.
